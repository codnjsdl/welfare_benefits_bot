import os
import json
import requests
import re

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain.globals import set_llm_cache
from langchain.cache import InMemoryCache
from PyPDF2 import PdfReader
from typing import Any, List, Mapping, Optional

import streamlit as st

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "loading_text" not in st.session_state:
    st.session_state.loading_text = None
if "retriever" not in st.session_state:
    st.session_state.retriever = None
if "vectorstore_cache" not in st.session_state:
    st.session_state.vectorstore_cache = {}

st.set_page_config(
    page_title="AKì•„ì´ì—ìŠ¤ ë³µì§€ì œë„ ì•Œë¦¬ë¯¸",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

class LlmClovaStudio(LLM):
    """
    Custom LLM class for using the ClovaStudio API.
    """
    host: str
    api_key: str
    api_key_primary_val: str
    request_id: str

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.host = kwargs.get('host')
        self.api_key = kwargs.get('api_key')
        self.api_key_primary_val = kwargs.get('api_key_primary_val')
        self.request_id = kwargs.get('request_id')

    @property
    def _llm_type(self) -> str:
        return "custom"

    def _call(self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None
    ) -> str:
        """
        Make an API call to the ClovaStudio endpoint using the specified
        prompt and return the response.
        """
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")

        headers = {
            "X-NCP-CLOVASTUDIO-API-KEY": self.api_key,
            "X-NCP-APIGW-API-KEY": self.api_key_primary_val,
            "X-NCP-CLOVASTUDIO-REQUEST-ID": self.request_id,
            "Content-Type": "application/json; charset=utf-8",
            "Accept": "text/event-stream"
        }

        sys_prompt = """ë‹¹ì‹ ì€ AKì•„ì´ì—ìŠ¤ì˜ ì‚¬ë‚´ ê·œì • ë° ì—…ë¬´ ê°€ì´ë“œì— ëŒ€í•œ ì „ë¬¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì—­í• ì„ ë§¡ê³  ìˆìŠµë‹ˆë‹¤.
        - íšŒì‚¬ì˜ ê·œì •, ì •ì±…, ì—…ë¬´ ê°€ì´ë“œì— ëŒ€í•´ì„œë§Œ ë‹µë³€í•©ë‹ˆë‹¤.
        - Contextì— ìˆëŠ” ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ì¶”ì¸¡í•˜ê±°ë‚˜ Context ì™¸ì˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        - ë‹µë³€ì€ ìµœëŒ€í•œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ, í•µì‹¬ ì •ë³´ë§Œì„ ì „ë‹¬í•©ë‹ˆë‹¤.
        - ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ê±°ë‚˜ Contextì— ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°, 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.
        - ì¼ìƒì ì¸ ëŒ€í™”ë‚˜ ì—…ë¬´ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì™„ê³¡í•˜ê²Œ ê±°ì ˆí•©ë‹ˆë‹¤.
        - ëŒ€í™”ì˜ íë¦„ì„ ëŠì§€ ì•Šìœ¼ë©´ì„œë„ ì¹œì ˆí•˜ê²Œ ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŒì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
        - ì˜ˆë¥¼ ë“¤ì–´, ì—…ë¬´ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì´ ë“¤ì–´ì˜¬ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•©ë‹ˆë‹¤: "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ì—…ë¬´ì— ê´€ë ¨ëœ ë‚´ìš©ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ë‹µë³€ì— ëŒ€í•œ ê°ì‚¬ ì¸ì‚¬, ì¹­ì°¬ì— ëŒ€í•´ì„œëŠ” 'ê°ì‚¬í•©ë‹ˆë‹¤!'ë¼ê³  ëŒ€ë‹µí•©ë‹ˆë‹¤. ë¶€ê°€ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
        """

        preset_text = [{"role": "system", "content": sys_prompt}, {"role": "user", "content": prompt}]

        request_data = {
            "messages": preset_text,
            "topP": 0.6,
            "topK": 0,
            "maxTokens": 256,
            "temperature": 0.3,
            "repeatPenalty": 1.2,
            "stopBefore": [],
            "includeAiFilters": False
        }

        response = requests.post(
            self.host + "/testapp/v1/chat-completions/HCX-003",
            headers=headers,
            json=request_data,
            stream=True
        )

        # ìŠ¤íŠ¸ë¦¼ì—ì„œ ë§ˆì§€ë§‰ 'data:' ë¼ì¸ì„ ì°¾ê¸° ìœ„í•œ ë¡œì§
        last_data_content = ""

        for line in response.iter_lines():
            if line:
                decoded_line = line.decode("utf-8")
                if '"data":"[DONE]"' in decoded_line:
                    break
                if decoded_line.startswith("data:"):
                    last_data_content = json.loads(decoded_line[5:])["message"]["content"]

        return last_data_content

llm = LlmClovaStudio(
    host='https://clovastudio.stream.ntruss.com',
    api_key='NTA0MjU2MWZlZTcxNDJiY6+5UNhJXWh3gqmFLbiMpde7ehpEJAFPwFUIey9lGc0S',
    api_key_primary_val='VhkfehtF14qpXmZPIA6VRw6x1c1eDCXp3P6BfbrG',
    request_id='b9288b57-8e12-45cc-b378-49cc13d8dbb6'
)

def extract_text_from_pdf(pdf_path, start_page=None, end_page=None):
    with open(pdf_path, 'rb') as file:
        reader = PdfReader(file)
        text = ""

        if not start_page:
            start_page = 0
        else:
            start_page -= 1

        if not end_page or end_page > len(reader.pages):
            end_page = len(reader.pages)

        for page_num in range(start_page, end_page):
            text += reader.pages[page_num].extract_text()

    return text

def retrieve_docs(text, model_index=0):
    model_list = [
        'bespin-global/klue-sroberta-base-continue-learning-by-mnr',
        'BAAI/bge-m3',
        'All-MiniLM-L6-v2',
        'sentence-transformers/paraphrase-MiniLM-L6-v2'
    ]

    if 'embeddings' not in st.session_state.vectorstore_cache:
        embeddings = HuggingFaceEmbeddings(
            model_name=model_list[model_index],
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True},
        )
        st.session_state.vectorstore_cache['embeddings'] = embeddings
    else:
        embeddings = st.session_state.vectorstore_cache['embeddings']

    vectorstore_path = 'vectorstore_' + str(model_index)
    os.makedirs(vectorstore_path, exist_ok=True)

    if vectorstore_path not in st.session_state.vectorstore_cache:
        if os.path.exists(os.path.join(vectorstore_path, "index")):
            vectorstore = Chroma(persist_directory=vectorstore_path, embedding_function=embeddings)
        else:
            docs = [Document(page_content=text)]
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)
            splits = text_splitter.split_documents(docs)
            vectorstore = Chroma.from_documents(splits, embeddings, persist_directory=vectorstore_path)
            vectorstore.persist()

        st.session_state.vectorstore_cache[vectorstore_path] = vectorstore
    else:
        vectorstore = st.session_state.vectorstore_cache[vectorstore_path]

    return vectorstore.as_retriever()

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def rag_chain(question):
    pdf_path = "AKì•„ì´ì—ìŠ¤ ë³µë¦¬í›„ìƒ ì œë„ ë§¤ë‰´ì–¼.pdf"

    if st.session_state.loading_text is None:
        loading_text = extract_text_from_pdf(pdf_path)
        st.session_state.loading_text = loading_text
    else:
        loading_text = st.session_state.loading_text

    if st.session_state.retriever is None:
        retriever = retrieve_docs(loading_text)
        st.session_state.retriever = retriever
    else:
        retriever = st.session_state.retriever

    if isinstance(retriever, str):
        return f"Error: {retriever}"

    retrieved_docs = retriever.get_relevant_documents(question)
    formatted_context = format_docs(retrieved_docs)
    formatted_prompt = f"Question: {question}\n\nContext: {formatted_context}"

    response = llm.invoke(formatted_prompt)
    return response

def generate_response(question):
    result = rag_chain(question)
    result = re.sub("(Answer:|ë‹µë³€:|ë‹µë³€|:|Answer)", "", result)
    return result

set_llm_cache(InMemoryCache())
st.title("AKì•„ì´ì—ìŠ¤ ë³µë¦¬í›„ìƒì œë„ ì•Œë¦¬ë¯¸")
st.markdown("í˜„ì¬ ì±—ë´‡ì€ **í…ŒìŠ¤íŠ¸ìš©**ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì— ì°¸ê³  ë°”ëë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    if not prompt.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
        msg = generate_response(prompt)
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.chat_message("assistant").write(msg)
